{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHSOcChlL6fx"
      },
      "source": [
        "##This file is used to create wordembeddings of roman urdu dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGUqgwcsL7NR"
      },
      "source": [
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSmqKpmyJ7VY"
      },
      "source": [
        "df = pd.read_csv(\"combined_data_1.csv\", header=None, error_bad_lines=False, lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "K_1hZazKelrU",
        "outputId": "a849f245-ea10-4858-c825-34af26c7acc5"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Make that dream home a reality with Bank Alfal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Will bank be open tomorrow?\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Procedure please?\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tariq Hanif Hani\\r</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I have had really bad experience with Alfalah ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0\n",
              "0  Make that dream home a reality with Bank Alfal...\n",
              "1                      Will bank be open tomorrow?\\r\n",
              "2                                Procedure please?\\r\n",
              "3                                 Tariq Hanif Hani\\r\n",
              "4  I have had really bad experience with Alfalah ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1ivcwQ0gPHf",
        "outputId": "af2ffa49-a5e8-4674-b0e5-c7efdcdc9572"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124156, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXZxAdWMgVFh"
      },
      "source": [
        "# preprocessing to remove currupt data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZnz8WpSgWc4"
      },
      "source": [
        "#converting empty values to nan\n",
        "import numpy as np\n",
        "df = df.replace(r'^\\s*$', np.NaN, regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqD-ijTNgcjJ"
      },
      "source": [
        "df.dropna(inplace=True)  #to remove empty strings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flis4E3QgfJD",
        "outputId": "feb1f902-936e-458a-8a72-031a3f2cb820"
      },
      "source": [
        "df.shape     #originally there were ~124000 entries, rest were corrupted."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(122924, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxXSpheGhNIW",
        "outputId": "e8131970-63df-4e3b-f1a7-345457da885e"
      },
      "source": [
        "mkdir data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWH5vnxiil-j"
      },
      "source": [
        "df.to_csv(r'data/data.csv', index = False)  #savig the files with uncorrupted data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "114nIaugi661"
      },
      "source": [
        "df1 = pd.read_csv(\"data/data.csv\", header=None, error_bad_lines=False, lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igw-vTgvjDRu",
        "outputId": "a48b7281-6a56-4036-ff0d-da6509ee40ca"
      },
      "source": [
        "df1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(122925, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biPDnqd8HTc9"
      },
      "source": [
        "# Run From Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb6jwMPIjDVJ",
        "outputId": "73ec836c-e2e5-485c-c6df-d3313dced657"
      },
      "source": [
        "!wget -c  https://raw.githubusercontent.com/rumaisamansoor/roman_urdu/main/data.txt -P data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-09 05:01:15--  https://raw.githubusercontent.com/rumaisamansoor/roman_urdu/main/data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8869712 (8.5M) [text/plain]\n",
            "Saving to: ‘data/data.txt’\n",
            "\n",
            "data.txt            100%[===================>]   8.46M  30.4MB/s    in 0.3s    \n",
            "\n",
            "2021-02-09 05:01:16 (30.4 MB/s) - ‘data/data.txt’ saved [8869712/8869712]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ATRK9jtjT_v"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "df = pd.read_csv(\"/content/data/data.txt\", header=None, error_bad_lines=False, lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCI3xCe1jUCE"
      },
      "source": [
        "def clean_text(mystring):\n",
        "  return re.sub('[^A-Za-z0-9 ]+', '', str(mystring)) \n",
        "  df[0] = df[0].astype(str)\n",
        "df[0] = df[0].apply(clean_text)\n",
        "df[0].to_csv(\"all.data\", sep='\\n', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLUz7fSBjUFn",
        "outputId": "1f0e3c81-7ca3-4230-b0b7-cb023f483731"
      },
      "source": [
        "df[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                         0\n",
              "1         Make that dream home a reality with Bank Alfal...\n",
              "2                                Will bank be open tomorrow\n",
              "3                                          Procedure please\n",
              "4                                          Tariq Hanif Hani\n",
              "                                ...                        \n",
              "122936    Kaash hum b parhay likhay hotayKabhi likhtay g...\n",
              "122937    Bahi sayasat kufrrr ha saaaf bttttt ha qanon s...\n",
              "122938                       aanti toh gussa e kr gai hain \n",
              "122939    mai b sirf shadi kanry ki waja say imran khan ...\n",
              "122940                                                     \n",
              "Name: 0, Length: 122941, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3fn-jiKj0Tk"
      },
      "source": [
        "## Fasttext setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awESpJxdj26u",
        "outputId": "2a75c7e8-2284-495a-d417-c5ac8d7038c5"
      },
      "source": [
        "cd ../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yX9OA7rjzcb",
        "outputId": "b0e3777a-ade2-4862-fe66-0acb226f3622"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3854, done.\u001b[K\n",
            "remote: Total 3854 (delta 0), reused 0 (delta 0), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3854/3854), 8.22 MiB | 5.07 MiB/s, done.\n",
            "Resolving deltas: 100% (2417/2417), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCd-eSypkIPV",
        "outputId": "6c2f011c-52a1-4422-e2ee-be546d8fd219"
      },
      "source": [
        "cd fastText"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/fastText\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQQcMDsFkLBT",
        "outputId": "6d664fcb-c0f5-4971-87f8-d80aa5aedb9f"
      },
      "source": [
        "!sudo pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /fastText\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (53.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3084479 sha256=90d9ab0fcac9d51a4af981f185f533f0bec69458953a0664b37d6b3994fcde77\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ggudv1o0/wheels/dd/ab/3c/3d74e0601246cef39c2174e7995f715a8e871e0173c6d99bf8\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "265tsKuZn-QQ"
      },
      "source": [
        "import fasttext\n",
        "#model = fasttext.train_supervised(input='train.data', autotuneValidationFile='val.data')\n",
        "model = fasttext.train_unsupervised(input='/content/data/data.txt',dim=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkX34FQ-n-Si",
        "outputId": "1ba02669-cc46-4b56-cacd-b5063ce3a4be"
      },
      "source": [
        "cd ../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mgIaTcan-V_"
      },
      "source": [
        "mkdir saved_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izJ1MGdQoMC2"
      },
      "source": [
        "model.save_model(\"saved_models/romanUrduFastText.bin\") #word embeddings saved as .bin file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJiNb6U9oMGU",
        "outputId": "695243d1-bd0d-4af4-9956-ff2defb6c52f"
      },
      "source": [
        "len(model.words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af_Q-nySoP9J",
        "outputId": "f33cee74-37ba-4a68-87c9-18e96a933514"
      },
      "source": [
        "model.get_nearest_neighbors(\"chalo\",10)  # definitely works"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9683986306190491, 'chalyga'),\n",
              " (0.9596998691558838, 'chalai'),\n",
              " (0.957528293132782, 'chaL'),\n",
              " (0.9559482336044312, 'chalna'),\n",
              " (0.9508107304573059, 'chaly'),\n",
              " (0.9478150606155396, 'chalte'),\n",
              " (0.9448511600494385, 'chala'),\n",
              " (0.9416161775588989, 'chalty'),\n",
              " (0.9368090033531189, 'chalti'),\n",
              " (0.9284162521362305, 'chalata')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmCIL0SaoP_f",
        "outputId": "9f4f6a07-c665-4558-8e30-143e8132a70e"
      },
      "source": [
        "model.get_nearest_neighbors(\"acha\",10)  #conflict with bura"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9254610538482666, 'achaa'),\n",
              " (0.9194252490997314, 'achcha'),\n",
              " (0.9136348962783813, 'achha'),\n",
              " (0.8700661659240723, 'bohata'),\n",
              " (0.8594368100166321, 'kapra'),\n",
              " (0.858151912689209, 'sacha'),\n",
              " (0.853145956993103, 'bura'),\n",
              " (0.8460121154785156, 'iska'),\n",
              " (0.8418800234794617, 'achee'),\n",
              " (0.8416031002998352, 'macha')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03_AMPWyVq7w",
        "outputId": "374893dd-1a1b-4478-f921-da6ce6e85a81"
      },
      "source": [
        "model.get_nearest_neighbors(\"nahi\",10)  #conflict with bura"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9470661878585815, 'nahi?'),\n",
              " (0.9301610589027405, 'nahii'),\n",
              " (0.9282290935516357, 'nahi.'),\n",
              " (0.9118558168411255, 'ahi'),\n",
              " (0.9102221131324768, 'yahi'),\n",
              " (0.9069234728813171, 'nahin.?'),\n",
              " (0.9061093330383301, 'nahe'),\n",
              " (0.9027920961380005, 'horahi'),\n",
              " (0.9018731117248535, 'nahin'),\n",
              " (0.8967084288597107, 'kahi')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8j7kLpUVMD1"
      },
      "source": [
        "# Fast Vector Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MP8c1TYoQDE"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# predefined class to import fastvector\n",
        "class FastVector:\n",
        "    def __init__(self, vector_file='', transform=None):\n",
        "        self.word2id = {}\n",
        "        self.id2word = []\n",
        "\n",
        "        print('reading word vectors from %s' % vector_file)\n",
        "        with open(vector_file, 'r') as f:\n",
        "            (self.n_words, self.n_dim) = \\\n",
        "                (int(x) for x in f.readline().rstrip('\\n').split(' '))\n",
        "            self.embed = np.zeros((self.n_words, self.n_dim))\n",
        "            for i, line in enumerate(f):\n",
        "                elems = line.rstrip('\\n').split(' ')\n",
        "                self.word2id[elems[0]] = i\n",
        "                self.embed[i] = elems[1:self.n_dim+1]\n",
        "                self.id2word.append(elems[0])\n",
        "\n",
        "        if transform is not None:\n",
        "            print('Applying transformation to embedding')\n",
        "            self.apply_transform(transform)\n",
        "\n",
        "    def apply_transform(self, transform):\n",
        "        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n",
        "        self.embed = np.matmul(self.embed, transmat)\n",
        "\n",
        "    def export(self, outpath):\n",
        "\n",
        "        fout = open(outpath, \"w\")\n",
        "\n",
        "        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n",
        "        for token in self.id2word:\n",
        "            vector_components = [\"%.6f\" % number for number in self[token]]\n",
        "            vector_as_string = \" \".join(vector_components)\n",
        "\n",
        "            out_line = token + \" \" + vector_as_string + \"\\n\"\n",
        "            fout.write(out_line)\n",
        "\n",
        "        fout.close()\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def cosine_similarity(cls, vec_a, vec_b):\n",
        "        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
        "        return np.dot(vec_a, vec_b) / \\\n",
        "            (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
        "\n",
        "    def __contains__(self, key):\n",
        "        return key in self.word2id\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.embed[self.word2id[key]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnSP8XXtoXpW"
      },
      "source": [
        "from fasttext import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fe71Aflodsb"
      },
      "source": [
        "##.bin not accepted in alignment, converting to vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNvDQ7U3ogci"
      },
      "source": [
        "def save_embeddings(model, output_dir):\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  np.save(os.path.join(output_dir, \"embeddings\"), model.get_input_matrix())\n",
        "  with open(os.path.join(output_dir, \"vocabulary.txt\"), \"w\", encoding='utf-8') as f:\n",
        "    for word in model.get_words():\n",
        "      f.write(word+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN1X3L3fokUQ"
      },
      "source": [
        "import os\n",
        "\n",
        "ft = fasttext.load_model('saved_models/romanUrduFastText.bin')\n",
        "save_embeddings(ft, 'saved_models/romanUrduFastText')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_iPZIOronFO"
      },
      "source": [
        "def load_embeddings(output_dir):\n",
        "  input_matrix = np.load(os.path.join(output_dir, \"embeddings.npy\"))\n",
        "  words = []\n",
        "  with open(os.path.join(output_dir, \"vocabulary.txt\"), \"r\", encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "      words.append(line.rstrip())\n",
        "  return words, input_matrix\n",
        "\n",
        "vocabulary, embeddings = load_embeddings('saved_models/romanUrduFastText')  #words and their embeddings are stored"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlqpq2QpIqx",
        "outputId": "28103a31-8a41-492c-8792-0b24c503216f"
      },
      "source": [
        "len(vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFcwYT0oplO"
      },
      "source": [
        "f = load_model('saved_models/romanUrduFastText.bin')\n",
        "lines=[]\n",
        "\n",
        "# get all words from model\n",
        "words = f.get_words()\n",
        "\n",
        "with open(\"/content/data/roman.vec\",'w') as file_out:\n",
        "    \n",
        "    # the first line must contain number of total words and vector dimension\n",
        "    file_out.write(str(len(words)) + \" \" + str(f.get_dimension()) + \"\\n\")\n",
        "\n",
        "    # line by line, you append vectors to VEC file\n",
        "    for w in words:\n",
        "        v = f.get_word_vector(w)\n",
        "        vstr = \"\"\n",
        "        for vi in v:\n",
        "            vstr += \" \" + str(vi)\n",
        "        try:\n",
        "            file_out.write(w + vstr+'\\n')\n",
        "        except:\n",
        "            pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-8qGdDQVR9B"
      },
      "source": [
        "# Alignment of roman urdu and english words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLzCjHh3oeWE",
        "outputId": "43fbf3cd-6ce0-4256-ad27-abcfc2c23998"
      },
      "source": [
        "ur_dictionary = FastVector(vector_file='/content/data/roman.vec')\n",
        "eng_dictionary = FastVector(vector_file='/content/wiki.simple.vec')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading word vectors from /content/data/roman.vec\n",
            "reading word vectors from /content/wiki.simple.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYouJMkuic8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d687585e-e8a0-459a-9234-09443c6da190"
      },
      "source": [
        "ur_vector = ur_dictionary[\"acha\"]\n",
        "eng_vector = eng_dictionary[\"good\"]\n",
        "\n",
        "#print(len(ur_vector))\n",
        "print(FastVector.cosine_similarity(ur_vector, eng_vector))   #since not alligned, so poor score => 0.5 shows the max similarity (Cosine function)."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.07203877131274354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC2xG3AQS7PY"
      },
      "source": [
        "\n",
        "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
        "def normalized(a, axis=-1, order=2):\n",
        "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2==0] = 1\n",
        "    return a / np.expand_dims(l2, axis)\n",
        "\n",
        "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
        "    \"\"\"\n",
        "    Source and target dictionaries are the FastVector objects of\n",
        "    source/target languages. bilingual_dictionary is a list of \n",
        "    translation pair tuples [(source_word, target_word), ...].\n",
        "    \"\"\"\n",
        "    source_matrix = []\n",
        "    target_matrix = []\n",
        "\n",
        "    for (source, target) in bilingual_dictionary:\n",
        "        if source in source_dictionary and target in target_dictionary:\n",
        "            source_matrix.append(source_dictionary[source])\n",
        "            target_matrix.append(target_dictionary[target])\n",
        "\n",
        "    # return training matrices\n",
        "    return np.array(source_matrix), np.array(target_matrix)\n",
        "\n",
        "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
        "    \"\"\"\n",
        "    Source and target matrices are numpy arrays, shape\n",
        "    (dictionary_length, embedding_dimension). These contain paired\n",
        "    word vectors from the bilingual dictionary.\n",
        "    \"\"\"\n",
        "    # optionally normalize the training vectors\n",
        "    if normalize_vectors:\n",
        "        source_matrix = normalized(source_matrix)\n",
        "        target_matrix = normalized(target_matrix)\n",
        "\n",
        "    # perform the SVD\n",
        "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
        "    U, s, V = np.linalg.svd(product)\n",
        "\n",
        "    # return orthogonal transformation which aligns source language to the target\n",
        "    return np.matmul(U, V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q494buliTYMT"
      },
      "source": [
        "ur_words = set(ur_dictionary.word2id.keys())\n",
        "eng_words = set(eng_dictionary.word2id.keys())\n",
        "overlap = list(ur_words & eng_words)\n",
        "bilingual_dictionary = [(entry, entry) for entry in overlap]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VPJSM9U11k7"
      },
      "source": [
        "bilingual_dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O49eycOMTi16"
      },
      "source": [
        "# form the training matrices\n",
        "source_matrix, target_matrix = make_training_matrices(\n",
        "    eng_dictionary, ur_dictionary, bilingual_dictionary)\n",
        "\n",
        "# learn and apply the transformation\n",
        "transform = learn_transformation(source_matrix, target_matrix)\n",
        "eng_dictionary.apply_transform(transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ2SAzIwV2L2"
      },
      "source": [
        "# Testing the allignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBTpiCWaTpvp",
        "outputId": "be7b6305-5e48-4401-c7e8-a520a4f51c5c"
      },
      "source": [
        "eng_vector = eng_dictionary[\"no\"]\n",
        "ur_vector = ur_dictionary[\"nahi\"]\n",
        "print(FastVector.cosine_similarity(eng_vector, ur_vector))   # ~ 0.5 is the max similarity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3554781736239731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHIqewk68cA_",
        "outputId": "87c18774-1ffd-4527-beab-2160f043108e"
      },
      "source": [
        "ur_dictionary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.FastVector at 0x7f6efbd47518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UdxyTks19oM"
      },
      "source": [
        "#method used to locally download the file\n",
        "\n",
        "# txt_filename = 'output.txt'\n",
        "\n",
        "# with open(txt_filename, 'w') as f:\n",
        "#     for word, vec in ur_dictionary.items():\n",
        "#         f.write('{} {}\\n'.format(word, ' '.join(['{:e}'.format(item) for item in vec])))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eOvCgJ5VvZX",
        "outputId": "290e22ae-385f-42b9-99a2-93a2afea3854"
      },
      "source": [
        "eng_vector = eng_dictionary[\"no\"]\n",
        "ur_vector = ur_dictionary[\"nahii\"]\n",
        "print(FastVector.cosine_similarity(eng_vector, ur_vector)) # ~ 0.35 and 0.32 almost same as nahii and nahi almost map vectors."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.32150650752114224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFkyoWVxWT3t",
        "outputId": "560529cc-edba-475a-eae2-e63a88736459"
      },
      "source": [
        "eng_vector = eng_dictionary[\"like\"]\n",
        "ur_vector = ur_dictionary[\"chalo\"]\n",
        "print(FastVector.cosine_similarity(eng_vector, ur_vector)) # ~ less similar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.25846389882201515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRyNGtnDHKSB",
        "outputId": "f442d444-9a07-4be5-e93d-eb3c295d7723"
      },
      "source": [
        "eng_vector1 = eng_dictionary[\"good\"]\n",
        "eng_vector2 = eng_dictionary[\"dont\"]\n",
        "print(FastVector.cosine_similarity(eng_vector1,eng_vector2)) # ~ less similar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3802946841363317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeOF_1vRYGyA",
        "outputId": "b3fa7f5b-3e48-4ecd-dd15-33814109e021"
      },
      "source": [
        "ur_vector1 = ur_dictionary[\"acha\"]\n",
        "ur_vector2 = ur_dictionary[\"like\"]\n",
        "print(FastVector.cosine_similarity(ur_vector1,ur_vector2)) # ~ less similar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.44049919439173013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aRp3XUJYcAq",
        "outputId": "b5f5fe95-ecc6-4b26-836b-47f22b6215fd"
      },
      "source": [
        "ur_vector1 = ur_dictionary[\"like\"]\n",
        "eng_vector2 = eng_dictionary[\"like\"]\n",
        "print(FastVector.cosine_similarity(ur_vector1,eng_vector2)) # ~ less similar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6579712967633884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLt_P5lapn6"
      },
      "source": [
        "import fasttext\n",
        "model = fasttext.train_unsupervised('/content/data/roman.vec')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}