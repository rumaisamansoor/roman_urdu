{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aligned_vector_saving.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "83bnt2mi0WKg"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a2ut5Ol4O9Y",
        "outputId": "2884f7f8-0fb4-4f44-f374-fef399c3608f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7GX2cmH5DE0",
        "outputId": "984a33e0-1b18-4f93-d3e5-dbdc8f64fbd6"
      },
      "source": [
        "cd ../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhWH3P2g5E8k",
        "outputId": "33af93e0-b722-4ce8-f493-7d0089652bce"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3854, done.\u001b[K\n",
            "remote: Total 3854 (delta 0), reused 0 (delta 0), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3854/3854), 8.23 MiB | 15.18 MiB/s, done.\n",
            "Resolving deltas: 100% (2416/2416), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLTMCYbl5GBS",
        "outputId": "56514aff-a401-49c8-b46f-a1ad8f40986b"
      },
      "source": [
        "cd fastText"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/fastText\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdjrhOeu5GEY",
        "outputId": "ca9c2ca8-8047-4db7-c091-ec9d9fb983f7"
      },
      "source": [
        "!sudo pip install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /fastText\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (53.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3084869 sha256=e92db880f0eb5e887e54f00d842bcdc6d1c2c76682a79cb48a519055c8c3f445\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-up538kai/wheels/dd/ab/3c/3d74e0601246cef39c2174e7995f715a8e871e0173c6d99bf8\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vyu5PD05GHI"
      },
      "source": [
        "import fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o75pDFl16M7-"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# predefined class to import fastvector\n",
        "class FastVector:\n",
        "    def __init__(self, vector_file='', transform=None):\n",
        "        self.word2id = {}\n",
        "        self.id2word = []\n",
        "\n",
        "        print('reading word vectors from %s' % vector_file)\n",
        "        with open(vector_file, 'r') as f:\n",
        "            (self.n_words, self.n_dim) = \\\n",
        "                (int(x) for x in f.readline().rstrip('\\n').split(' '))\n",
        "            self.embed = np.zeros((self.n_words, self.n_dim))\n",
        "            for i, line in enumerate(f):\n",
        "                elems = line.rstrip('\\n').split(' ')\n",
        "                self.word2id[elems[0]] = i\n",
        "                self.embed[i] = elems[1:self.n_dim+1]\n",
        "                self.id2word.append(elems[0])\n",
        "\n",
        "        if transform is not None:\n",
        "            print('Applying transformation to embedding')\n",
        "            self.apply_transform(transform)\n",
        "\n",
        "    def apply_transform(self, transform):\n",
        "        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n",
        "        self.embed = np.matmul(self.embed, transmat)\n",
        "\n",
        "    def export(self, outpath):\n",
        "\n",
        "        fout = open(outpath, \"w\")\n",
        "\n",
        "        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n",
        "        for token in self.id2word:\n",
        "            vector_components = [\"%.6f\" % number for number in self[token]]\n",
        "            vector_as_string = \" \".join(vector_components)\n",
        "\n",
        "            out_line = token + \" \" + vector_as_string + \"\\n\"\n",
        "            fout.write(out_line)\n",
        "\n",
        "        fout.close()\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def cosine_similarity(cls, vec_a, vec_b):\n",
        "        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
        "        return np.dot(vec_a, vec_b) / \\\n",
        "            (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
        "\n",
        "    def __contains__(self, key):\n",
        "        return key in self.word2id\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.embed[self.word2id[key]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJY9uvD34O_s",
        "outputId": "93b198c5-8db9-4bf7-bfbf-286e9dffd1c9"
      },
      "source": [
        "from fasttext import load_model\n",
        "\n",
        "ur_dictionary = FastVector(vector_file='/content/gdrive/MyDrive/Data_for_wordEmbeddings/roman.vec')\n",
        "eng_dictionary = FastVector(vector_file='/content/gdrive/MyDrive/Data_for_wordEmbeddings/try_wiki.simple.vec')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading word vectors from /content/gdrive/MyDrive/Data_for_wordEmbeddings/roman.vec\n",
            "reading word vectors from /content/gdrive/MyDrive/Data_for_wordEmbeddings/try_wiki.simple.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFohrqJf4PCJ",
        "outputId": "3a4f6b21-0384-423f-a0ad-3c1e51b01a4a"
      },
      "source": [
        "ur_vector = ur_dictionary[\"acha\"]\n",
        "eng_vector = eng_dictionary[\"good\"]\n",
        "\n",
        "#print(len(ur_vector))\n",
        "print(FastVector.cosine_similarity(ur_vector, eng_vector))   #since not alligned, so poor score => 0.5 shows the max similarity (Cosine function)."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.07203877131274354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpExBAc64PEt"
      },
      "source": [
        "\n",
        "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
        "def normalized(a, axis=-1, order=2):\n",
        "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2==0] = 1\n",
        "    return a / np.expand_dims(l2, axis)\n",
        "\n",
        "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
        "    \"\"\"\n",
        "    Source and target dictionaries are the FastVector objects of\n",
        "    source/target languages. bilingual_dictionary is a list of \n",
        "    translation pair tuples [(source_word, target_word), ...].\n",
        "    \"\"\"\n",
        "    source_matrix = []\n",
        "    target_matrix = []\n",
        "\n",
        "    for (source, target) in bilingual_dictionary:\n",
        "        if source in source_dictionary and target in target_dictionary:\n",
        "            source_matrix.append(source_dictionary[source])\n",
        "            target_matrix.append(target_dictionary[target])\n",
        "\n",
        "    # return training matrices\n",
        "    return np.array(source_matrix), np.array(target_matrix)\n",
        "\n",
        "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
        "    \"\"\"\n",
        "    Source and target matrices are numpy arrays, shape\n",
        "    (dictionary_length, embedding_dimension). These contain paired\n",
        "    word vectors from the bilingual dictionary.\n",
        "    \"\"\"\n",
        "    # optionally normalize the training vectors\n",
        "    if normalize_vectors:\n",
        "        source_matrix = normalized(source_matrix)\n",
        "        target_matrix = normalized(target_matrix)\n",
        "\n",
        "    # perform the SVD\n",
        "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
        "    U, s, V = np.linalg.svd(product)\n",
        "\n",
        "    # return orthogonal transformation which aligns source language to the target\n",
        "    return np.matmul(U, V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU25FEBX6igY"
      },
      "source": [
        "ur_words = set(ur_dictionary.word2id.keys())\n",
        "eng_words = set(eng_dictionary.word2id.keys())\n",
        "overlap = list(ur_words & eng_words)\n",
        "bilingual_dictionary = [(entry, entry) for entry in overlap]\n",
        "# print(eng_dictionary.word2id.keys())\n",
        "# eng_words = np.array(eng_words)\n",
        "# print(eng_words(0))\n",
        "\n",
        "\n",
        "eng_words_list = []\n",
        "for i in eng_words:\n",
        "  eng_words_list.append(i)\n",
        "eng_words = np.array(eng_words_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHh3L_kb7ifv",
        "outputId": "d81d91d7-953a-4664-92ca-5a47b4772eeb"
      },
      "source": [
        "len(eng_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "111048"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYPX4p4w6rWb"
      },
      "source": [
        "# form the training matrices\n",
        "source_matrix, target_matrix = make_training_matrices(\n",
        "    eng_dictionary, ur_dictionary, bilingual_dictionary)\n",
        "\n",
        "# learn and apply the transformation\n",
        "transform = learn_transformation(source_matrix, target_matrix)\n",
        "eng_dictionary.apply_transform(transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWACmXaD6rZ6",
        "outputId": "ca0fd1d4-ed12-41f7-dbf0-cb022ea8ee4b"
      },
      "source": [
        "#before\n",
        "eng_vector = eng_dictionary[\"no\"]\n",
        "ur_vector = ur_dictionary[\"nahi\"]\n",
        "print(FastVector.cosine_similarity(eng_vector, ur_vector))  # ~ 0.5 is the max similarity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35547817362397727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyW1AXQ_6ijQ",
        "outputId": "3f6dea45-e38e-4343-c951-0f75a1ace283"
      },
      "source": [
        "#with urdu transform\n",
        "\n",
        "eng_vector = eng_dictionary[\"no\"]\n",
        "ur_vector = ur_dictionary[\"nahi\"]\n",
        "print(FastVector.cosine_similarity(eng_vector, ur_vector))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35547817362397727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY210swL6imk",
        "outputId": "23897599-96a3-487e-e04a-6b4bfc4deb2c"
      },
      "source": [
        "eng_vector = eng_dictionary[\"no\"]\n",
        "ur_vector = ur_dictionary[\"han\"]\n",
        "print(FastVector.cosine_similarity(eng_vector, ur_vector))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.23291628691549376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5huvFGjZ_AUe"
      },
      "source": [
        "len(eng_words)\n",
        "\n",
        "file = open(\"/content/gdrive/MyDrive/Data_for_wordEmbeddings/aligned_vectors.vec\", \"w\")\n",
        "\n",
        "file.write(str(len(eng_words))+' '+ str(300) +'\\n')\n",
        "\n",
        "\n",
        "temp=''\n",
        "for i in range(len(eng_words)):\n",
        "  temp=temp + eng_words[i] + ' '\n",
        "  for j in eng_dictionary[eng_words[i]]:\n",
        "      # print(j ,\" \") \n",
        "      temp = temp + str(j) + ' '\n",
        "  file.write(temp)\n",
        "  file.write(\"\\n\")\n",
        "  temp=''\n",
        "\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nppcSr4Kj8wW",
        "outputId": "849cfc6d-74cf-486e-b97d-44c9c9c25d74"
      },
      "source": [
        "test_dictionary = FastVector(vector_file='/content/gdrive/MyDrive/Data_for_wordEmbeddings/aligned_vectors.vec')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading word vectors from /content/gdrive/MyDrive/Data_for_wordEmbeddings/aligned_vectors.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrVJMHglj8z7",
        "outputId": "f4700ba4-8c37-4ecb-fe77-0575b98fd7fc"
      },
      "source": [
        "test_vector = test_dictionary[\"no\"]\n",
        "ur_vector = ur_dictionary[\"han\"]\n",
        "print(FastVector.cosine_similarity(test_vector, ur_vector))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.23291628691549376\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}